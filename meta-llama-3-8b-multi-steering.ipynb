{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sae-lens transformer-lens plotly pandas numpy scipy tqdm\n",
    "!pip install sae-lens\n",
    "!pip install transformer-lens\n",
    "!pip install plotly\n",
    "!pip install -U bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, re\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ========================================\n",
    "# 0) 환경 설정\n",
    "# ========================================\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from huggingface_hub import login\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"✅ Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"⚠️ HF_TOKEN not set, skipping login\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# ========================================\n",
    "# 1) 모델 & SAE 로드\n",
    "# ========================================\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(f\"\\nLoading model: {MODEL_NAME}\")\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device=device,\n",
    "    dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    ")\n",
    "print(\"✅ Model loaded!\")\n",
    "\n",
    "release = \"llama-3-8b-it-res-jh\"\n",
    "sae_id  = \"blocks.25.hook_resid_post\"\n",
    "\n",
    "print(f\"\\nLoading SAE: release={release}, sae_id={sae_id}\")\n",
    "sae, cfg_dict, _ = SAE.from_pretrained(release, sae_id, device=device)\n",
    "print(\"✅ SAE loaded!\")\n",
    "\n",
    "# SAE 설정값 파싱\n",
    "HOOK_NAME   = sae_id                                                        # \"blocks.25.hook_resid_post\"\n",
    "LAYER_IDX   = int(re.search(r\"blocks\\.(\\d+)\\.\", sae_id).group(1))          # 25\n",
    "PREPEND_BOS = getattr(sae.cfg, \"prepend_bos\", True)\n",
    "\n",
    "print(f\"   Hook:       {HOOK_NAME}\")\n",
    "print(f\"   Layer:      {LAYER_IDX}\")\n",
    "print(f\"   d_in:       {sae.cfg.d_in}\")\n",
    "print(f\"   d_sae:      {sae.cfg.d_sae}\")\n",
    "print(f\"   prepend_bos:{PREPEND_BOS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}